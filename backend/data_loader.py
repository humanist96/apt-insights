"""
ë°ì´í„° ë¡œë” ëª¨ë“ˆ
output ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ì„ ë¡œë“œí•˜ê±°ë‚˜ PostgreSQLì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
USE_DATABASE í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë“œ ì „í™˜ (True: DB, False: JSON)
"""
import os
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import re
import traceback
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False, JSON ëª¨ë“œ)
USE_DATABASE = os.getenv('USE_DATABASE', 'False').lower() == 'true'

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œì¼ ë•Œë§Œ import
if USE_DATABASE:
    try:
        from .db.repository import TransactionRepository
        from .db.session import get_session
        DATABASE_AVAILABLE = True
    except ImportError as e:
        print(f"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   JSON ëª¨ë“œë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        DATABASE_AVAILABLE = False
        USE_DATABASE = False
else:
    DATABASE_AVAILABLE = False


def _get_field_value(item: Dict, *keys: str) -> str:
    for key in keys:
        value = item.get(key, '')
        if value:
            return value
    return ''


def load_all_json_data(base_path: Optional[Path] = None, debug: bool = False) -> Tuple[List[Dict], Dict]:
    """
    ë°ì´í„° ë¡œë“œ (Dual-Mode: JSON ë˜ëŠ” PostgreSQL)

    USE_DATABASE í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ ë°ì´í„° ì†ŒìŠ¤ ê²°ì •:
    - True: PostgreSQLì—ì„œ ë¡œë“œ
    - False (ê¸°ë³¸ê°’): JSON íŒŒì¼ì—ì„œ ë¡œë“œ

    ì£¼ì˜: ì‹¤ì œ JSON íŒŒì¼ì—ì„œë§Œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ëª©ì—… ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    Args:
        base_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (Noneì´ë©´ í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©)
        debug: ë””ë²„ê¹… ì •ë³´ ë°˜í™˜ ì—¬ë¶€

    Returns:
        (items ë¦¬ìŠ¤íŠ¸, ë””ë²„ê¹… ì •ë³´ ë”•ì…”ë„ˆë¦¬)
        - items: ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        - debug_info: ë””ë²„ê¹… ì •ë³´ (ì„±ê³µ/ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡, ì˜¤ë¥˜ ë©”ì‹œì§€ ë“±)
    """
    # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œ
    if USE_DATABASE and DATABASE_AVAILABLE:
        print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œ: PostgreSQLì—ì„œ ë°ì´í„° ë¡œë“œ")
        return _load_from_database()

    # JSON ëª¨ë“œ (ê¸°ë³¸ê°’)
    print("ğŸ“ JSON ëª¨ë“œ: íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ")
    return _load_from_json(base_path, debug)


def _load_from_database() -> Tuple[List[Dict], Dict]:
    """
    PostgreSQLì—ì„œ ë°ì´í„° ë¡œë“œ

    Returns:
        (items ë¦¬ìŠ¤íŠ¸, ë””ë²„ê¹… ì •ë³´ ë”•ì…”ë„ˆë¦¬)
    """
    try:
        with get_session() as session:
            repository = TransactionRepository(session)
            items, debug_info = repository.load_all_transactions()

            # ëª¨ë“œ í‘œì‹œ
            debug_info['data_source'] = 'postgresql'
            debug_info['use_database'] = True

            return items, debug_info

    except Exception as e:
        print(f"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("   JSON ëª¨ë“œë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        return _load_from_json(None, False)


def _load_from_json(base_path: Optional[Path] = None, debug: bool = False) -> Tuple[List[Dict], Dict]:
    """
    JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ ë¡œì§)

    Args:
        base_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
        debug: ë””ë²„ê¹… ì •ë³´ ë°˜í™˜ ì—¬ë¶€

    Returns:
        (items ë¦¬ìŠ¤íŠ¸, ë””ë²„ê¹… ì •ë³´ ë”•ì…”ë„ˆë¦¬)
    """
    if base_path is None:
        # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
        current_file = Path(__file__)
        base_path = current_file.parent.parent
    
    all_items = []
    debug_info = {
        'successful_files': [],
        'failed_files': [],
        'total_files': 0,
        'total_items': 0,
        'errors': []
    }
    
    # ëª¨ë“  api_XX/output ë””ë ‰í† ë¦¬ì—ì„œ test_results JSON íŒŒì¼ ì°¾ê¸°
    json_files = []
    for api_dir in base_path.glob('api_*/output'):
        json_files.extend(list(api_dir.glob('*test_results*.json')))
    
    debug_info['total_files'] = len(json_files)
    
    for json_file in json_files:
        try:
            file_size_mb = json_file.stat().st_size / (1024 * 1024)
            
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            file_items_count = 0
            
            # test_results ë°°ì—´ì—ì„œ items ì¶”ì¶œ
            test_results = data.get('test_results', [])
            for test_result in test_results:
                result = test_result.get('result', {})
                if not result.get('error', False):
                    items = result.get('items', [])
                    if items:
                        # ê° itemì— API íƒ€ì… ì •ë³´ ì¶”ê°€
                        api_type = json_file.parent.parent.name  # api_01, api_02 ë“±
                        for item in items:
                            item['_api_type'] = api_type
                            item['_source_file'] = str(json_file)
                        all_items.extend(items)
                        file_items_count += len(items)
            
            debug_info['successful_files'].append({
                'file': str(json_file),
                'size_mb': round(file_size_mb, 2),
                'items_count': file_items_count
            })
            debug_info['total_items'] += file_items_count
            
        except json.JSONDecodeError as e:
            error_msg = f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}"
            debug_info['failed_files'].append({
                'file': str(json_file),
                'error': error_msg,
                'error_type': 'JSONDecodeError'
            })
            debug_info['errors'].append({
                'file': str(json_file),
                'error': error_msg,
                'traceback': traceback.format_exc() if debug else None
            })
            if debug:
                print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ [{json_file.name}]: {e}")
            continue
        except MemoryError as e:
            error_msg = f"ë©”ëª¨ë¦¬ ë¶€ì¡±: {str(e)}"
            debug_info['failed_files'].append({
                'file': str(json_file),
                'error': error_msg,
                'error_type': 'MemoryError'
            })
            debug_info['errors'].append({
                'file': str(json_file),
                'error': error_msg,
                'traceback': traceback.format_exc() if debug else None
            })
            if debug:
                print(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡± [{json_file.name}]: {e}")
            continue
        except Exception as e:
            error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
            debug_info['failed_files'].append({
                'file': str(json_file),
                'error': error_msg,
                'error_type': type(e).__name__
            })
            debug_info['errors'].append({
                'file': str(json_file),
                'error': error_msg,
                'traceback': traceback.format_exc() if debug else None
            })
            if debug:
                print(f"âŒ ì˜¤ë¥˜ [{json_file.name}]: {e}")
            continue
    
    return all_items, debug_info


def remove_duplicates(items: List[Dict]) -> List[Dict]:
    """
    ì¤‘ë³µëœ ê±°ë˜ ë°ì´í„° ì œê±°
    
    ì¤‘ë³µ íŒë‹¨ ê¸°ì¤€:
    - aptSeq + dealYear + dealMonth + dealDay + dealAmount
    - ë˜ëŠ” aptNm + umdNm + dealYear + dealMonth + dealDay + dealAmount (aptSeqê°€ ì—†ëŠ” ê²½ìš°)
    
    Args:
        items: ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¤‘ë³µì´ ì œê±°ëœ ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    seen = set()
    unique_items = []
    
    for item in items:
        # aptSeqê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        apt_seq = _get_field_value(item, 'aptSeq', 'apt')
        apt_nm = _get_field_value(item, 'aptNm', 'ì•„íŒŒíŠ¸')
        umd_nm = _get_field_value(item, 'umdNm', 'ë²•ì •ë™', 'ë²•ì •ë™ëª…')
        deal_year = _get_field_value(item, 'dealYear', 'ë…„')
        deal_month = _get_field_value(item, 'dealMonth', 'ì›”')
        deal_day = _get_field_value(item, 'dealDay', 'ì¼')
        deal_amount = _get_field_value(item, 'dealAmount', 'ê±°ë˜ê¸ˆì•¡')
        
        # ê³ ìœ  í‚¤ ìƒì„±
        if apt_seq:
            key = (
                str(apt_seq),
                str(deal_year),
                str(deal_month),
                str(deal_day),
                str(deal_amount)
            )
        else:
            # aptSeqê°€ ì—†ìœ¼ë©´ aptNm + umdNm ì¡°í•© ì‚¬ìš©
            key = (
                str(apt_nm),
                str(umd_nm),
                str(deal_year),
                str(deal_month),
                str(deal_day),
                str(deal_amount)
            )
        
        if key not in seen:
            seen.add(key)
            unique_items.append(item)
    
    return unique_items


def normalize_data(items: List[Dict]) -> List[Dict]:
    """
    ë°ì´í„° ì •ê·œí™”
    
    - ê±°ë˜ê¸ˆì•¡: ë¬¸ìì—´(ì‰¼í‘œ í¬í•¨) â†’ ìˆ«ì (ë§Œì› ë‹¨ìœ„)
    - ë©´ì : ë¬¸ìì—´ â†’ float
    - ë‚ ì§œ: year, month, day â†’ datetime ê°ì²´ ë° YYYY-MM-DD ë¬¸ìì—´
    - ì§€ì—­ëª…: sggNm, umdNm ì¡°í•©
    
    Args:
        items: ì›ë³¸ ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì •ê·œí™”ëœ ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    normalized = []
    
    for item in items:
        normalized_item = item.copy()
        
        # ê±°ë˜ê¸ˆì•¡ ì •ê·œí™” (ë§Œì› ë‹¨ìœ„ ìˆ«ìë¡œ ë³€í™˜)
        deal_amount = _get_field_value(item, 'dealAmount', 'ê±°ë˜ê¸ˆì•¡')
        if deal_amount:
            try:
                # ì‰¼í‘œ ì œê±° í›„ ìˆ«ì ë³€í™˜
                amount_str = str(deal_amount).replace(',', '').strip()
                if amount_str:
                    normalized_item['_deal_amount_numeric'] = float(amount_str)
                else:
                    normalized_item['_deal_amount_numeric'] = None
            except (ValueError, AttributeError):
                normalized_item['_deal_amount_numeric'] = None
        else:
            normalized_item['_deal_amount_numeric'] = None
        
        # ë©´ì  ì •ê·œí™”
        area = _get_field_value(item, 'excluUseAr', 'ì „ìš©ë©´ì ', 'ë©´ì ')
        if area:
            try:
                area_str = str(area).replace(',', '').strip()
                if area_str:
                    normalized_item['_area_numeric'] = float(area_str)
                else:
                    normalized_item['_area_numeric'] = None
            except (ValueError, AttributeError):
                normalized_item['_area_numeric'] = None
        else:
            normalized_item['_area_numeric'] = None
        
        # ë‚ ì§œ ì •ê·œí™”
        year = _get_field_value(item, 'dealYear', 'ë…„')
        month = _get_field_value(item, 'dealMonth', 'ì›”')
        day = _get_field_value(item, 'dealDay', 'ì¼')
        
        if year and month and day:
            try:
                year_int = int(str(year))
                month_int = int(str(month))
                day_int = int(str(day))
                
                # datetime ê°ì²´ ìƒì„±
                deal_date = datetime(year_int, month_int, day_int)
                normalized_item['_deal_date'] = deal_date
                normalized_item['_deal_date_str'] = f"{year_int}-{month_int:02d}-{day_int:02d}"
                normalized_item['_deal_year_month'] = f"{year_int}-{month_int:02d}"
            except (ValueError, TypeError):
                normalized_item['_deal_date'] = None
                normalized_item['_deal_date_str'] = None
                normalized_item['_deal_year_month'] = None
        else:
            normalized_item['_deal_date'] = None
            normalized_item['_deal_date_str'] = None
            normalized_item['_deal_year_month'] = None
        
        # ì§€ì—­ëª… ì •ê·œí™”
        sgg_nm = _get_field_value(item, 'sggNm', 'ì‹œêµ°êµ¬')
        umd_nm = _get_field_value(item, 'umdNm', 'ë²•ì •ë™', 'ë²•ì •ë™ëª…')
        
        if sgg_nm or umd_nm:
            normalized_item['_region_name'] = f"{sgg_nm} {umd_nm}".strip()
        else:
            normalized_item['_region_name'] = None
        
        # ì¸µìˆ˜ ì •ê·œí™”
        floor = _get_field_value(item, 'floor', 'ì¸µ', 'ì¸µìˆ˜')
        if floor:
            try:
                # ì¸µìˆ˜ ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                floor_str = re.sub(r'[^0-9-]', '', str(floor))
                if floor_str:
                    normalized_item['_floor_numeric'] = int(floor_str)
                else:
                    normalized_item['_floor_numeric'] = None
            except (ValueError, TypeError):
                normalized_item['_floor_numeric'] = None
        else:
            normalized_item['_floor_numeric'] = None
        
        # ê±´ì¶•ë…„ë„ ì •ê·œí™”
        build_year = _get_field_value(item, 'buildYear', 'ê±´ì¶•ë…„ë„', 'ê±´ì¶•ì—°ë„')
        if build_year:
            try:
                normalized_item['_build_year_numeric'] = int(str(build_year))
            except (ValueError, TypeError):
                normalized_item['_build_year_numeric'] = None
        else:
            normalized_item['_build_year_numeric'] = None
        
        normalized.append(normalized_item)
    
    return normalized


def filter_by_region(items: List[Dict], region_name: Optional[str] = None) -> List[Dict]:
    """
    ì§€ì—­ë³„ë¡œ ë°ì´í„° í•„í„°ë§
    
    Args:
        items: ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        region_name: ì§€ì—­ëª… (Noneì´ë©´ í•„í„°ë§ ì•ˆ í•¨)
    
    Returns:
        í•„í„°ë§ëœ ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not region_name:
        return items
    
    region_name_lower = region_name.lower()
    filtered = []
    
    for item in items:
        # _region_nameì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ í•„ë“œ í™•ì¸
        region = item.get('_region_name', '')
        if not region:
            sgg_nm = item.get('sggNm', '') or item.get('ì‹œêµ°êµ¬', '')
            umd_nm = item.get('umdNm', '') or item.get('ë²•ì •ë™', '') or item.get('ë²•ì •ë™ëª…', '')
            region = f"{sgg_nm} {umd_nm}".strip()
        
        if region_name_lower in region.lower():
            filtered.append(item)
    
    return filtered


def load_and_process_data(
    base_path: Optional[Path] = None,
    region_filter: Optional[str] = None,
    remove_dup: bool = True,
    debug: bool = False
) -> Tuple[List[Dict], Optional[Dict]]:
    """
    JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì¤‘ë³µ ì œê±° ë° ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” í†µí•© í•¨ìˆ˜
    
    Args:
        base_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
        region_filter: ì§€ì—­ í•„í„° (Noneì´ë©´ ëª¨ë“  ì§€ì—­)
        remove_dup: ì¤‘ë³µ ì œê±° ì—¬ë¶€
        debug: ë””ë²„ê¹… ì •ë³´ ë°˜í™˜ ì—¬ë¶€
    
    Returns:
        (ì²˜ë¦¬ëœ ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸, ë””ë²„ê¹… ì •ë³´) ë˜ëŠ” (ì²˜ë¦¬ëœ ê±°ë˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸, None)
    """
    # 1. JSON íŒŒì¼ ë¡œë“œ
    items, debug_info = load_all_json_data(base_path, debug=debug)
    
    # 2. ì¤‘ë³µ ì œê±°
    if remove_dup:
        before_count = len(items)
        items = remove_duplicates(items)
        after_count = len(items)
        if debug:
            debug_info['deduplication'] = {
                'before_count': before_count,
                'after_count': after_count,
                'removed_count': before_count - after_count
            }
    
    # 3. ì§€ì—­ í•„í„°ë§
    if region_filter:
        before_count = len(items)
        items = filter_by_region(items, region_filter)
        after_count = len(items)
        if debug:
            debug_info['region_filtering'] = {
                'region': region_filter,
                'before_count': before_count,
                'after_count': after_count
            }
    
    # 4. ë°ì´í„° ì •ê·œí™”
    items = normalize_data(items)
    
    if debug:
        return items, debug_info
    else:
        return items, None
